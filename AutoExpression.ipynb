{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感分析之表情适配\n",
    "\n",
    "使用RNN + WordEmbedding来建立一个表情适配器 \n",
    "\n",
    "我们经常喜欢在文字后面加上表情来使得我们的文字更加有表达性和感染力，但是每次输入完文字都需要我们花上一定的时间去挑选表情。而本项目的目的在于在用户输入完文字之后能够自动配上相应场景的表情，使得语句更具有表达性同时又节省了用户的时间与精力。比如在我们输完语句 \"Congratulations on the promotion! Lets get coffee and talk. Love you!\"之后，表情适配器能够自动在每句话后添加上表情，如： \"Congratulations on the promotion! 👍 Lets get coffee and talk. ☕️ Love you! ❤️\"\n",
    "\n",
    "我们将完成一个模型，在输入一个句子（如\"I want to play basketball!\"）之后，模型将会找到最匹配当前语句的表情（⚾️）加在语句的结尾处。在许多现有的模型中，模型将只会记得❤️是\"heart\"的标志，但当换了一个词例如\"love\"的时候，模型将不会识别出。而在本项目中，我们将使用词向量来表征每一个单词，使得算法能够泛化到训练集中没有出现的词汇中，如训练集中的❤️是\"heart\"的标志，当我们输入训练集中没有的词汇\"love\"时，❤️也会被自动添加上。因此，词向量的引入还允许我们使用一个较小的训练集来训练我们的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from expression_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 数据预处理\n",
    "\n",
    "### 1.1 - 数据集 EMOJISET\n",
    "\n",
    "我们有一个较小的数据集(X, Y)\n",
    "\n",
    "- X 含有127个句子（都是字符串）\n",
    "- Y 含有一个整数标签（0-4）分别对应每个句子所适用的表情\n",
    "\n",
    "<img src=\"images/data_set.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - 一个有着5个类的分类问题，这里给出了一些句子的例子</center></caption>\n",
    "\n",
    "划分数据集为训练集（127个例子）和测试集（56个例子） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train.csv')\n",
    "X_test, Y_test = read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印X_train中的某个句子和对应的标签，index可以自行改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love you mum ❤️\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使标签格式化，使用\"one-hot representation\"转换$Y$从维度$(m, 1)$到 $(m, 5)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看转换后Y标签的效果，index可以自行改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 转换为one hot向量 [1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 5\n",
    "print(Y_train[index], \"转换为one hot向量\", Y_oh_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "将输入句子转换成词向量表征形式，我们使用50维的GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/GloveWordEmbedding.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成3个python字典\n",
    "- `word_to_index`: 将每个单词映射为他们在词汇表所对应的索引（词汇表中共400001个单词，对应着索引0-400000）\n",
    "- `index_to_word`: 将每个索引映射为他们在词汇表所对应的单词\n",
    "- `word_to_vec_map`: 将每个单词映射为GloVe向量表征形式\n",
    "\n",
    "测试转换效果，index可以自行改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cucumber 在词汇表中的索引为 113317\n",
      "索引 289846在词汇表中对应的单词是 potatos\n"
     ]
    }
   ],
   "source": [
    "word = \"cucumber\"\n",
    "index = 289846\n",
    "print(word, \"在词汇表中的索引为\", word_to_index[word])\n",
    "print(\"索引\", str(index) + \"在词汇表中对应的单词是\", index_to_word[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 搭建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - 模型1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 - 模型结构\n",
    "<center>\n",
    "<img src=\"images/model1.png\" style=\"width:900px;height:300px;\">\n",
    "<caption><center> **Figure 2**: Average model.</center></caption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "   \n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    avg = np.zeros((50,))\n",
    "    \n",
    "    for w in words:\n",
    "        avg += word_to_vec_map[w]\n",
    "    avg = avg / len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试某个句子的均值词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg =  [-0.008005    0.56370833 -0.50427333  0.258865    0.55131103  0.03104983\n",
      " -0.21013718  0.16893933 -0.09590267  0.141784   -0.15708967  0.18525867\n",
      "  0.6495785   0.38371117  0.21102167  0.11301667  0.02613967  0.26037767\n",
      "  0.05820667 -0.01578167 -0.12078833 -0.02471267  0.4128455   0.5152061\n",
      "  0.38756167 -0.898661   -0.535145    0.33501167  0.68806933 -0.2156265\n",
      "  1.797155    0.10476933 -0.36775333  0.750785    0.10282583  0.348925\n",
      " -0.27262833  0.66768    -0.10706167 -0.283635    0.59580117  0.28747333\n",
      " -0.3366635   0.23393817  0.34349183  0.178405    0.1166155  -0.076433\n",
      "  0.1445417   0.09808667]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"Morrocan couscous is my favorite dish\", word_to_vec_map)\n",
    "print(\"avg = \", avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 - 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 50                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    # Initialize parameters using Xavier initialization\n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    # Convert Y to Y_onehot with n_y classes\n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    # Optimization loop\n",
    "    for t in range(num_iterations):                       # Loop over the number of iterations\n",
    "        for i in range(m):                                # Loop over the training examples\n",
    "            \n",
    "            # Average the word vectors of the words from the i'th training example\n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            # Forward propagate the avg through the softmax layer\n",
    "            z = W @ avg + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            # Compute cost using the i'th training label's one hot representation and \"A\" (the output of the softmax)\n",
    "            cost = -np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            # Update parameters with Stochastic Gradient Descent\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map)\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,)\n",
      "(132,)\n",
      "(132, 5)\n",
      "never talk to me again\n",
      "<class 'numpy.ndarray'>\n",
      "(20,)\n",
      "(20,)\n",
      "(132, 5)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(X_train[0])\n",
    "print(type(X_train))\n",
    "Y = np.asarray([5,0,0,5, 4, 4, 4, 6, 6, 4, 1, 1, 5, 6, 6, 3, 6, 3, 4, 4])\n",
    "print(Y.shape)\n",
    "\n",
    "X = np.asarray(['I am going to the bar tonight', 'I love you', 'miss you my dear',\n",
    " 'Lets go party and drinks','Congrats on the new job','Congratulations',\n",
    " 'I am so happy for you', 'Why are you feeling bad', 'What is wrong with you',\n",
    " 'You totally deserve this prize', 'Let us go play football',\n",
    " 'Are you down for football this afternoon', 'Work hard play harder',\n",
    " 'It is suprising how people can be dumb sometimes',\n",
    " 'I am very disappointed','It is the best day in my life',\n",
    " 'I think I will end up alone','My life is so boring','Good job',\n",
    " 'Great so awesome'])\n",
    "\n",
    "print(X.shape)\n",
    "print(np.eye(5)[Y_train.reshape(-1)].shape)\n",
    "print(type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.9520498812810072\n",
      "Accuracy: 0.3484848484848485\n",
      "Epoch: 100 --- cost = 0.07971818726014807\n",
      "Accuracy: 0.9318181818181818\n",
      "Epoch: 200 --- cost = 0.04456369243681402\n",
      "Accuracy: 0.9545454545454546\n",
      "Epoch: 300 --- cost = 0.03432267378786059\n",
      "Accuracy: 0.9696969696969697\n",
      "[[3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [4.]\n",
      " [1.]\n",
      " [4.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 - 测试性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.9772727272727273\n",
      "Test set:\n",
      "Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)\n",
    "print('Test set:')\n",
    "pred_test = predict(X_test, Y_test, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n",
      "\n",
      "i adore you ❤️\n",
      "i love you ❤️\n",
      "funny lol 😄\n",
      "lets play with a ball ⚾\n",
      "food is ready 🍴\n",
      "not feeling happy 😄\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"i adore you\", \"i love you\", \"funny lol\", \"lets play with a ball\", \"food is ready\", \"not feeling happy\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该算法有一个很大的缺陷，它并没有使得“not feeling happy”正确。 该算法忽略了单词排序，因此不善于理解“不开心”这样的短语。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印混淆矩阵还有助于了解哪些类对于模型理解更加困难。 混淆矩阵显示了标签是一个类（“实际”类）的示例经常被具有不同类（“预测”类）的算法错误标记的频率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56,)\n",
      "           ❤️    ⚾    😄    😞   🍴\n",
      "Predicted  0.0  1.0  2.0  3.0  4.0  All\n",
      "Actual                                 \n",
      "0            6    0    0    1    0    7\n",
      "1            0    8    0    0    0    8\n",
      "2            2    0   16    0    0   18\n",
      "3            1    1    2   12    0   16\n",
      "4            0    0    1    0    6    7\n",
      "All          9    9   19   13    6   56\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(Y_test.shape)\n",
    "print('           '+ label_to_emoji(0)+ '    ' + label_to_emoji(1) + '    ' +  label_to_emoji(2)+ '    ' + label_to_emoji(3)+'   ' + label_to_emoji(4))\n",
    "print(pd.crosstab(Y_test, pred_test.reshape(56,), rownames=['Actual'], colnames=['Predicted'], margins=True))\n",
    "plot_confusion_matrix(Y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - 模型2 \n",
    "### 使用Keras框架建立LSTMs单元：\n",
    "使用预训练的次嵌入表征每个单词，然后喂给LSTM单元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - 模型结构\n",
    "\n",
    "<img src=\"images/model2.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 3**: 2层LSTM序列分类器. </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 填充训练集语句\n",
    "\n",
    "对于一个选定的深度神经网络，我们需要每个输入的语句有着相同的长度，即有相同数量的单词。但是现在训练集中的句子长度不一，所以我们使用padding来填充每个句子。使用训练集所有句子中最长的句子的长度作为输入RNN的语句长度，长度低于最长长度的句子中剩余词向量就用零向量进行填充。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 - Embedding层\n",
    "\n",
    "Embedding层使用(batch size, max input length)作为输入。\n",
    "\n",
    "<img src=\"images/embedding1.png\" style=\"width:700px;height:250px;\">\n",
    "<caption><center> **Figure 4**: Embedding层. 这个例子展示了两个语句通过embedding层的前向传播过程。两个语句都用0填充到了长度5.最终的维度是`(2,max_len,50)`，因为我们使用的词嵌入是50维的。 </center></caption>\n",
    "\n",
    "\n",
    "第一步先将所有训练的句子转换为对应的索引列表，然后再用0填充长度不满max_len的句子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "   \n",
    "    m = X.shape[0]                                \n",
    "    \n",
    "\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                             \n",
    "        \n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "  \n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "        \n",
    "            j = j + 1\n",
    "            \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看 `sentences_to_indices()` 效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 = ['everyday is nice' 'let us play baseball' 'food is ready for you']\n",
      "X1_indices = [[141944. 192973. 260760.      0.      0.]\n",
      " [220870. 374021. 286375.  69714.      0.]\n",
      " [151204. 192973. 302254. 151349. 394475.]]\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([\"everyday is nice\", \"let us play baseball\", \"food is ready for you\"])\n",
    "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
    "print(\"X1 =\", X1)\n",
    "print(\"X1_indices =\", X1_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Kreas中建立`Embedding()` 层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    " \n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                 \n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      \n",
    "    \n",
    "  \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    " \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix]) # 对应后面的[][][]三个维度\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.4 建立模型\n",
    "\n",
    "建立模型，将embedding层的输出喂给LSTM网络\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:700px;height:400px;\"> <br>\n",
    "<caption><center> **Figure 5**: 一个两层的LSTM序列分类器 </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMModel(input_shape, word_to_vec_map, word_to_index):\n",
    "\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    embeddings = embedding_layer(sentence_indices)  \n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "   \n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "   \n",
    "    X = Dropout(0.5)(X)\n",
    "    \n",
    "    X = Dense(5)(X)\n",
    "    \n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译模型，使用交叉熵损失函数，并将准确率作为评估指标 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代50次，每代每个批次的数据大小为32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1922 - acc: 0.9545\n",
      "Epoch 2/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0972 - acc: 0.9697\n",
      "Epoch 3/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1390 - acc: 0.9394\n",
      "Epoch 4/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1015 - acc: 0.9697\n",
      "Epoch 5/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0793 - acc: 0.9773\n",
      "Epoch 6/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0530 - acc: 0.9848\n",
      "Epoch 7/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0362 - acc: 0.9924\n",
      "Epoch 8/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0303 - acc: 0.9924\n",
      "Epoch 9/50\n",
      "132/132 [==============================] - ETA: 0s - loss: 0.0242 - acc: 1.000 - 0s 1ms/step - loss: 0.0279 - acc: 1.0000\n",
      "Epoch 10/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 11/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0128 - acc: 1.0000\n",
      "Epoch 12/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0145 - acc: 1.0000\n",
      "Epoch 13/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0097 - acc: 1.0000\n",
      "Epoch 14/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0089 - acc: 1.0000\n",
      "Epoch 15/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0074 - acc: 1.0000\n",
      "Epoch 16/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 17/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 18/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 31/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 32/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 39/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0014 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 44/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0016 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e02b0c860>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看测试集准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 6ms/step\n",
      "\n",
      "Test accuracy =  0.8392857228006635\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看错误分类情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:😄 prediction: she got me a nice present\t❤️\n",
      "Expected emoji:😞 prediction: This girl is messing with me\t❤️\n",
      "Expected emoji:😞 prediction: work is horrible\t😄\n",
      "Expected emoji:🍴 prediction: any suggestions for dinner\t😄\n",
      "Expected emoji:❤️ prediction: I love taking breaks\t😞\n",
      "Expected emoji:😄 prediction: you brighten my day\t❤️\n",
      "Expected emoji:😞 prediction: she is a bully\t😄\n",
      "Expected emoji:😄 prediction: will you be my valentine\t❤️\n",
      "Expected emoji:😞 prediction: go away\t⚾\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试自己的测试语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am hungry 🍴\n"
     ]
    }
   ],
   "source": [
    "inputSentence = \"I am hungry\"\n",
    "x_test = np.array([inputSentence])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model/my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试自己的测试语句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am very glad to see you 😄\n"
     ]
    }
   ],
   "source": [
    "inputSentence = \"I am very glad to see you\"\n",
    "x_test = np.array([inputSentence])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
